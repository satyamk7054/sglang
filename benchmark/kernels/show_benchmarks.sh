#!/bin/bash
# Quick Reference: MoE Quantization Benchmarks
# Run this to see available benchmarks

echo "================================================================================"
echo "                MoE Quantization Benchmarks for H200"
echo "================================================================================"
echo ""
echo "Question: Should I use FP8 or MXFP4 for GPT-OSS on H200?"
echo ""
echo "================================================================================"
echo "QUICK START - Run this first:"
echo "================================================================================"
echo ""
echo "  python benchmark/kernels/simple_moe_quant_bench.py --batch-size 1024 --tp-size 4"
echo ""
echo "================================================================================"
echo "Available Benchmarks:"
echo "================================================================================"
echo ""
echo "1. SIMPLE (Recommended for first run)"
echo "   Uses: SGLang's fused_moe kernel"
echo "   Speed: Fast to run"
echo "   Command:"
echo "     python benchmark/kernels/simple_moe_quant_bench.py \\"
echo "         --batch-size 1024 --tp-size 4"
echo ""
echo "2. DETAILED (Advanced users)"
echo "   Uses: Direct matmul_ogs kernel"
echo "   Speed: More thorough"
echo "   Command:"
echo "     python benchmark/kernels/moe_quant_comparison.py \\"
echo "         --model-config gpt-oss-120b --tp-size 4"
echo ""
echo "3. REAL MODEL (Most realistic)"
echo "   Uses: Actual model configuration"
echo "   Speed: Same as simple"
echo "   Command:"
echo "     python benchmark/kernels/bench_gpt_oss_moe_kernel.py \\"
echo "         --model openai/gpt-oss-120b --tp-size 4"
echo ""
echo "================================================================================"
echo "Common Options:"
echo "================================================================================"
echo ""
echo "  --batch-size N      Number of tokens (default: 1024)"
echo "  --tp-size N         Tensor parallelism (default: 1, use 4 for H200)"
echo "  --num-iters N       Benchmark iterations (default: 100)"
echo "  --formats fp8 mxfp4 Which formats to test"
echo ""
echo "================================================================================"
echo "Documentation:"
echo "================================================================================"
echo ""
echo "  Full guide:  benchmark/kernels/MoE_QUANT_BENCHMARK.md"
echo "  Summary:     benchmark/kernels/BENCHMARK_SUMMARY.md"
echo ""
echo "================================================================================"
echo "Expected Result on H200:"
echo "================================================================================"
echo ""
echo "  ✓ FP8 should be 1.3-1.8x FASTER than MXFP4"
echo "  ✗ FP8 uses 2x MORE MEMORY than MXFP4"
echo ""
echo "  Recommendation: Use FP8 if you have sufficient memory"
echo ""
echo "================================================================================"
echo ""
